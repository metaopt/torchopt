{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for zero-order differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/5_Implicit_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f9865-dc02-43d4-be90-da1160c4e4dd",
   "metadata": {},
   "source": [
    "By treating the solution $\\phi^{\\star}$ as an implicit function of $\\theta$, the idea of implicit differentiation is to directly get analytical best-response derivatives $\\partial \\phi^{\\star}(\\theta)/ \\partial \\theta$ by implicit function theorem. This is suitable for algorithms when the inner-level optimal solution is achieved ${\\left. \\frac{\\partial F (\\phi, \\theta)}{\\partial \\phi} \\right\\rvert}_{\\phi = \\phi^{\\star}} = 0$ or reaches some stationary conditions $F (\\phi^{\\star}, \\theta) = 0$, such as [IMAML](https://arxiv.org/abs/1909.04630) and [DEQ](https://arxiv.org/abs/1909.01377)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4b9e1-115f-45ad-a9b3-ea338bcfe6dd",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to conduct implicit differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Functional API\n",
    "\n",
    "The basic functional API is `torchopt.diff.implicit.custom_root`, which is used as the decorator for the forward process implicit gradient procedures. Users are required to implement the stationary conditions for the inner-loop process, which will be used as the input of custom_root decorator. We show the pseudo code in the following part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4400b-a491-4f07-926c-c421ac5a2069",
   "metadata": {},
   "source": [
    "```python\n",
    "# Functional API for implicit gradient\n",
    "def stationary(params, meta_params, data):\n",
    "    # stationary condition construction\n",
    "    return stationary condition\n",
    "\n",
    "# Decorator that wraps the function\n",
    "# Optionally specify the linear solver (conjugate gradient or Neumann series)\n",
    "@torchopt.diff.implicit.custom_root(stationary, solve=linear_solver)\n",
    "def solve(params, meta_params, data):\n",
    "    # Forward optimization process for params\n",
    "    return optimal_params\n",
    "\n",
    "# Define params, meta params and get data\n",
    "params, meta_prams, data = ..., ..., ...\n",
    "optimal_params = solve(params, meta_params, data)\n",
    "loss = outer_loss(optimal_params)\n",
    "\n",
    "meta_grads = torch.autograd.grad(loss, meta_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "Here we use the example of [IMAML](https://arxiv.org/abs/1909.04630) as a real example. For IMAML, the inner-loop objective is described by the following equation.\n",
    "\n",
    "$$\n",
    "{\\mathcal{Alg}}^{\\star} \\left( \\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\text{tr}} \\right) = \\underset{\\phi'}{\\operatorname{\\arg \\min}} ~ G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\triangleq \\mathcal{L} \\left( \\boldsymbol{\\phi}', \\mathcal{D}_{i}^{\\text{tr}} \\right) + \\frac{\\lambda}{2} {\\left\\| \\boldsymbol{\\phi}' - \\boldsymbol{\\theta} \\right\\|}^{2}\n",
    "$$\n",
    "\n",
    "According to this function, we can define the forward function `inner_solver`, where we solve this equation based on sufficient gradient descents. For such inner-loop process, the optimality condition is that the gradient w.r.t inner-loop parameter is $0$.\n",
    "\n",
    "$$\n",
    "{\\left. \\nabla_{\\boldsymbol{\\phi}'} G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\right\\rvert}_{\\boldsymbol{\\phi}' = \\boldsymbol{\\phi}^{\\star}} = 0\n",
    "$$\n",
    "\n",
    "Thus we can define the optimality function by defining `imaml_objective` and make it first-order gradient w.r.t the inner-loop parameter as $0$. We achieve so by calling out `functorch.grad(imaml_objective, argnums=0)`. Finally, the forward function is decorated by the `@torchopt.diff.implicit.custom_root` decorator and the optimality condition we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-1682.1396, -1721.7643,  -374.3944,  -648.2670,  1268.0751,  1033.9446,\n",
      "          -472.1609, -3160.3914,   481.5174, -1887.5736,   522.9168,   460.3890,\n",
      "           179.0576,  1849.2092,  1668.5990,  -369.4632, -2021.0269, -2533.9246,\n",
      "           846.6440,  1185.5966,   894.7379, -2323.4985,  -510.0334,  2768.6133,\n",
      "          1120.8730,  -874.8035,  -259.0755,   274.1380,  2075.8796,  2370.1736,\n",
      "          1413.8832, -1260.5542]]), tensor([-846.1462]))\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2022 MetaOPT Team. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import functorch\n",
    "import torch\n",
    "\n",
    "import torchopt\n",
    "\n",
    "sigma = 0.01\n",
    "a = torch.nn.Linear(32, 1)\n",
    "fmodel, params = functorch.make_functional(a)\n",
    "x = torch.randn(64, 32)\n",
    "y = torch.randn(64)\n",
    "distribution = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "\n",
    "\n",
    "@torchopt.diff.zero_order.zero_order(distribution, sigma=0.001)\n",
    "def forward_process(params, f, x, y):\n",
    "    y_pred = f(params, x)\n",
    "    loss = torch.mean((y - y_pred) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "out = forward_process(params, fmodel, x, y)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "print(torch.autograd.grad(out, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75c81-d479-4120-a73d-5b2b488358d0",
   "metadata": {},
   "source": [
    "In the next step, we consider a specific case for one layer neural network to fit the linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89189f0f-ea6f-44e1-9083-7bfe15d1e8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m forward_process(params, fmodel, x, y)\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchopt/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchopt/lib/python3.9/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/TorchOpt/torchopt/diff/zero_order.py:109\u001b[0m, in \u001b[0;36m_zero_order_naive.<locals>.apply.<locals>.ZeroOrder.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_perturbation\u001b[39m(tensor, noise):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39madd(noise, alpha\u001b[38;5;241m=\u001b[39msigma)\n\u001b[0;32m--> 109\u001b[0m noise \u001b[38;5;241m=\u001b[39m [distribution\u001b[38;5;241m.\u001b[39msample(sample_shape\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m flat_diff_params]\n\u001b[1;32m    110\u001b[0m flat_noisy_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m     add_perturbation(t, n) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_diff_params, noise)\n\u001b[1;32m    112\u001b[0m ]\n\u001b[1;32m    113\u001b[0m noisy_params: List[Any] \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     diff_params_treespec, flat_noisy_params\n\u001b[1;32m    115\u001b[0m )\n",
      "File \u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/TorchOpt/torchopt/diff/zero_order.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_perturbation\u001b[39m(tensor, noise):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39madd(noise, alpha\u001b[38;5;241m=\u001b[39msigma)\n\u001b[0;32m--> 109\u001b[0m noise \u001b[38;5;241m=\u001b[39m [\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m(sample_shape\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m flat_diff_params]\n\u001b[1;32m    110\u001b[0m flat_noisy_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    111\u001b[0m     add_perturbation(t, n) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_diff_params, noise)\n\u001b[1;32m    112\u001b[0m ]\n\u001b[1;32m    113\u001b[0m noisy_params: List[Any] \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     diff_params_treespec, flat_noisy_params\n\u001b[1;32m    115\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "def sample(shape):\n",
    "    x = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "    return x.sample(shape)\n",
    "\n",
    "\n",
    "@torchopt.diff.zero_order.zero_order(sample, sigma=0.001)\n",
    "def forward_process(params, f, x, y):\n",
    "    y_pred = f(params, x)\n",
    "    loss = torch.mean((y - y_pred) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "out = forward_process(params, fmodel, x, y)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "print(torch.autograd.grad(out, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95538b-1fd9-4ec8-9f57-6360bedc05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(20, 4)\n",
    "w = torch.randn(4, 1)\n",
    "b = torch.randn(1)\n",
    "y = x @ w + b + 0.5 * torch.randn(20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1823a-2231-4471-bb68-cce7724f2578",
   "metadata": {},
   "source": [
    "We instantiate an one layer neural network, where the weights and bias are initialized with constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50a7bfe-ac69-4089-8cf8-3cbd69d6d4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1, bias=True)\n",
    "        nn.init.ones_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Net(4)\n",
    "fmodel, meta_params = functorch.make_functional(model)\n",
    "data = (x, y, fmodel)\n",
    "\n",
    "# Clone function for parameters\n",
    "def clone(params):\n",
    "    cloned = []\n",
    "    for item in params:\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            cloned.append(item.clone().detach_().requires_grad_(True))\n",
    "        else:\n",
    "            cloned.append(item)\n",
    "    return tuple(cloned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c36c4-89e2-4a63-8213-63db6ee3b08e",
   "metadata": {},
   "source": [
    "We take the forward process by calling out the forward function, then we pass the optimal params into the outer-loop loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115e79c6-911f-4743-a2ed-e50a71c3a813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_params = inner_solver(clone(meta_params), meta_params, data)\n",
    "\n",
    "outer_loss = fmodel(optimal_params, x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812351-f635-496e-9732-c80831ac04a6",
   "metadata": {},
   "source": [
    "Finally, we can get the meta gradient as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdcbe8d-2336-4f80-b124-eb43c5a2fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outer_loss, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ae8bb",
   "metadata": {},
   "source": [
    "Also we can switch to the Neumann Series inversion linear solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43df0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "optimal_params = inner_solver_inv_ns(clone(meta_params), meta_params, data)\n",
    "outer_loss = fmodel(optimal_params, x).mean()\n",
    "torch.autograd.grad(outer_loss, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e67ea-b220-4a14-a1ea-4eb3c5f52b6b",
   "metadata": {},
   "source": [
    "## 2. OOP API\n",
    "\n",
    "The basic OOP class is the class `ImplicitMetaGradientModule`. We make the network as an `nn.Module` following a classical PyTorch style. Users need to define the stationary condition/objective function and the inner-loop solve function to enable implicit gradient computation. We show the pseudo code in the following part.\n",
    "\n",
    "```python\n",
    "from torchopt.diff.implicit import ImplicitMetaGradientModule\n",
    "\n",
    "# Inherited from the class ImplicitMetaGradientModule\n",
    "# Optionally specify the linear solver (conjugate gradient or Neumann series)\n",
    "class InnerNet(ImplicitMetaGradientModule, linear_solve=linear_solver):\n",
    "    def __init__(self, meta_module):\n",
    "        ...\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Forward process\n",
    "        ...\n",
    "\n",
    "    def optimality(self, batch, labels):\n",
    "        # Stationary condition construction for calculating implicit gradient\n",
    "        # NOTE: If this method is not implemented, it will be automatically derived from the\n",
    "        # gradient of the `objective` function.\n",
    "        ...\n",
    "\n",
    "    def objective(self, batch, labels):\n",
    "        # Define the inner-loop optimization objective\n",
    "        # NOTE: This method is optional if method `optimality` is implemented.\n",
    "        ...\n",
    "\n",
    "    def solve(self, batch, labels):\n",
    "        # Conduct the inner-loop optimization\n",
    "        ...\n",
    "        return self  # optimized module\n",
    "\n",
    "# Get meta_params and data\n",
    "meta_params, data = ..., ...\n",
    "inner_net = InnerNet()\n",
    "\n",
    "# Solve for inner-loop process related with the meta parameters\n",
    "optimal_inner_net = inner_net.solve(meta_params, *data)\n",
    "\n",
    "# Get outer-loss and solve for meta-gradient\n",
    "loss = outer_loss(optimal_inner_net)\n",
    "meta_grad = torch.autograd.grad(loss, meta_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3999684-f4d3-4bc0-86ab-a7e803b2fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "from torchopt.diff.implicit import ImplicitMetaGradientModule\n",
    "\n",
    "\n",
    "class InnerNet(\n",
    "    ImplicitMetaGradientModule,\n",
    "    linear_solve=torchopt.linear_solve.solve_normal_cg(maxiter=5, atol=0),\n",
    "):\n",
    "    def __init__(self, meta_net, n_inner_iter, reg_param):\n",
    "        super().__init__()\n",
    "        self.meta_net = meta_net\n",
    "        # Get a deepcopy\n",
    "        self.net = torchopt.module_clone(meta_net, by='deepcopy', detach_buffers=True)\n",
    "        self.n_inner_iter = n_inner_iter\n",
    "        self.reg_param = reg_param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def objective(self, x, y):\n",
    "        # We do not implement the optimality conditions, so it will be automatically derived from\n",
    "        # the gradient of the `objective` function.\n",
    "        y_pred = self(x)\n",
    "        loss = F.mse_loss(y_pred, y)\n",
    "        regularization_loss = 0\n",
    "        for p1, p2 in zip(\n",
    "            self.parameters(),  # parameters of `self.net`\n",
    "            self.meta_parameters(),  # parameters of `self.meta_net`\n",
    "        ):\n",
    "            regularization_loss += (\n",
    "                0.5 * self.reg_param * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "            )\n",
    "        return loss + regularization_loss\n",
    "\n",
    "    def solve(self, x, y):\n",
    "        params = tuple(self.parameters())\n",
    "        inner_optim = torchopt.SGD(params, lr=2e-2)\n",
    "        with torch.enable_grad():\n",
    "            # Temporarily enable gradient computation for conducting the optimization\n",
    "            for _ in range(self.n_inner_iter):\n",
    "                loss = self.objective(x, y)\n",
    "                inner_optim.zero_grad()\n",
    "                # NOTE: The parameter inputs should be explicitly specified in `backward` function\n",
    "                # as argument `inputs`. Otherwise, if not provided, the gradient is accumulated into\n",
    "                # all the leaf Tensors (including the meta-parameters) that were used to compute the\n",
    "                # objective output. Alternatively, please use `torch.autograd.grad` instead.\n",
    "                loss.backward(inputs=params)  # backward pass in inner-loop\n",
    "                inner_optim.step()  # update inner parameters\n",
    "        return self\n",
    "\n",
    "\n",
    "# Initialize the meta network\n",
    "meta_net = Net(4)\n",
    "inner_net = InnerNet(meta_net, 100, reg_param=1)\n",
    "\n",
    "# Solve for inner-loop\n",
    "optimal_inner_net = inner_net.solve(x, y)\n",
    "outer_loss = optimal_inner_net(x).mean()\n",
    "\n",
    "# Derive the meta gradient\n",
    "torch.autograd.grad(outer_loss, meta_net.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchopt",
   "language": "python",
   "name": "torchopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8cc1ff2cbc47027bf9993941710d9ab9175f14080903d9c7c432ee63d681da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
