{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for zero-order differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/5_Implicit_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f9865-dc02-43d4-be90-da1160c4e4dd",
   "metadata": {},
   "source": [
    "When the inner-loop process is non-differentiable or one wants to eliminate the heavy computation burdens in the previous two modes (brought by Hessian), one can choose ZD. ZD typically gets gradients based on zero-order estimation, such as finite-difference, or Evolutionary Strategy.\n",
    "\n",
    "TorchOpt offers API for ES-based differentiation. Instead of optimizing the objective $F$, ES optimizes a Gaussion smoothing objective defined as $\\tilde{f}_\\sigma({\\theta}) = \\mathbb{E}_{{z} \\sim \\mathcal{N}( {0}, {I}_d )} \\sim[ f ({\\theta} + \\sigma {z}) ]$, where $\\sigma$ denotes precision. The gradient of such objective is $\\nabla_\\theta \\tilde{f}_\\sigma(\\theta) = \\frac{1}{\\sigma} \\mathbb{E}_{{z} \\sim \\mathcal{N}( {0}, {I}_d )} \\sim[ f({\\theta} + \\sigma {z}) {z} ]$. Refer to [ES-MAML](https://arxiv.org/pdf/1910.01215.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4b9e1-115f-45ad-a9b3-ea338bcfe6dd",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to ES-based differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 MetaOPT Team. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Functional API\n",
    "\n",
    "The basic functional API is `torchopt.diff.zero_order.zero_order`, which is used as the decorator for the forward process zero-order gradient procedures. Users are required to implement the noise sampling function, which will be used as the input of zero_order decorator. Here we show the speicifc meaning for each parameter used in the decorator.\n",
    "* distribution for noise sampling distribution\n",
    "* sigma is for precision\n",
    "* method for different kind of algorithms, we support 'naive'([ES-RL](https://arxiv.org/abs/1703.03864)), 'forward'([Forward-FD](http://proceedings.mlr.press/v80/choromanski18a/choromanski18a.pdf)) and 'antithetic'([antithetic](https://d1wqtxts1xzle7.cloudfront.net/75609515/coredp2011_1web-with-cover-page-v2.pdf?Expires=1670215467&Signature=RfP~mQhhhI7aGknwXbRBgSggFrKuNTPYdyUSdMmfTxOa62QoOJAm-Xhr3F1PLyjUQc2JVxmKIKGGuyYvyfCTpB31dfmMtuVQxZMWVF-SfErTN05SliC93yjA1x1g2kjhn8bkBFdQqGl~1RQSKnhj88BakgSeDNzyCxwbD5VgR89BXRs4YIK5RBIKYtgLhoyz5jar7wHS3TJhRzs3WNeTIAjAmLqJ068oGFZ0Jr7maGquTe3w~8LEEIprJ6cyCMc6b1UUJkmwjNq0RLTVbxgFjfi4Z9kyxyJB9IOS1J25OOON4jfwh5JlXS7MVskuONUyHJim1TQ8OwCraKlBsQLPQw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)).\n",
    "* argnums specifies which parameter we want to trace the meta-gradient.\n",
    "* num_samples specifies how many times we want to conduct the sampling.\n",
    "\n",
    "We show the pseudo code in the following part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4400b-a491-4f07-926c-c421ac5a2069",
   "metadata": {},
   "source": [
    "```python\n",
    "# Functional API for implicit gradient\n",
    "# customize \n",
    "class distributions():\n",
    "    def sample(shape):\n",
    "        # sampling function for noise\n",
    "        return noise\n",
    "\n",
    "distribution = distributions()\n",
    "\n",
    "# distribution can also be torch.distributions, e.g., torch.distributions.normal.Normal()\n",
    "\n",
    "# Decorator that wraps the function\n",
    "@torchopt.diff.zero_order.zero_order(distribution=distribution, sigma=0.01, method='naive', argnums=1, num_samples=100)\n",
    "def solve(params, meta_params, data):\n",
    "    # Forward optimization process for params\n",
    "    return optimal_params\n",
    "\n",
    "# Define params, meta params and get data\n",
    "params, meta_prams, data = ..., ..., ...\n",
    "optimal_params = solve(params, meta_params, data)\n",
    "loss = outer_loss(optimal_params)\n",
    "\n",
    "meta_grads = torch.autograd.grad(loss, meta_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "Here we use the example of a linear layer as an example, note that this is just an example to show linear layer can work with ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0269, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0246, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0225, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0205, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0187, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0171, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0156, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0144, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0134, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0128, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0122, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0120, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0121, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n"
     ]
    }
   ],
   "source": [
    "import functorch\n",
    "import torch\n",
    "\n",
    "import torchopt\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "a = torch.nn.Linear(32, 1)\n",
    "fmodel, params = functorch.make_functional(a)\n",
    "x = torch.randn(64, 32) * 0.1\n",
    "y = torch.randn(64) * 0.1\n",
    "distribution = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "\n",
    "\n",
    "@torchopt.diff.zero_order.zero_order(distribution=distribution, sigma=0.01, method='forward', argnums=0, num_samples=1000)\n",
    "def forward_process(params, f, x, y):\n",
    "    y_pred = f(params, x)\n",
    "    loss = torch.mean((y - y_pred) ** 2)\n",
    "    return loss\n",
    "\n",
    "#out = forward_process(params, fmodel, x, y)\n",
    "optimizer = torchopt.adam(lr=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_state = optimizer.init(params)                       # init optimizer                             \n",
    "    loss = forward_process(params, fmodel, x, y)             # compute loss\n",
    "\n",
    "    grads = torch.autograd.grad(loss, params)                # compute gradients\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)  # get updates\n",
    "    params = torchopt.apply_updates(params, updates)         # update network parameters\n",
    "    \n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchopt",
   "language": "python",
   "name": "torchopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8cc1ff2cbc47027bf9993941710d9ab9175f14080903d9c7c432ee63d681da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
