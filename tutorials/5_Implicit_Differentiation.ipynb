{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for implicit differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/5_Implicit_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01214059-78d9-4cde-9b08-007c2eb078b3",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to conduct implicit differentiation. Here we use the example of [IMAML](https://arxiv.org/abs/1909.04630) as the illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "\n",
    "import torchopt\n",
    "from torchopt import implicit_diff, sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Basic API\n",
    "\n",
    "The basic API is **implicit_diff**, which is used as the decorator for the forward process implicit gradient procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0a5b5a-8189-446e-ac9c-0594470df3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchopt import implicit_diff, sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "## 1.1 Forward Process, Backward Process with Optimality Conditions\n",
    "For IMAML, the inner-loop objective is described by the following equation.\n",
    "\n",
    "$$\n",
    "\\mathcal{A} l g^{\\star}\\left(\\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\operatorname{tr}}\\right)=\\underset{\\phi^{\\prime} \\in \\Phi}{\\operatorname{argmin}} \\mathcal{L}\\left(\\boldsymbol{\\phi}^{\\prime}, \\mathcal{D}_{i}^{\\operatorname{tr}}\\right)+\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\phi}^{\\prime}-\\boldsymbol{\\theta}\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "According to this function, we can define the forward function **inner_solver**, where we solve this equation based on sufficient gradient descents. For such inner-loop process, the optimality condition is that the gradient w.r.t inner-loop parameter is 0.\n",
    "\n",
    "$$\n",
    "\\left.\\nabla_{\\boldsymbol{\\phi}^{\\prime}} G\\left(\\boldsymbol{\\phi}^{\\prime}, \\boldsymbol{\\theta}\\right)\\right|_{\\phi^{\\prime}=\\boldsymbol{\\phi}}=0\n",
    "$$\n",
    "\n",
    "Thus we can define the optimality function by defining **imaml_objective** and make it first-order gradient w.r.t the inner-loop parameter as 0. We achieve so by calling out **functorch.grad(imaml_objective, argnums=0)**. Finally, the forward function is decorated by the **@implicit_diff** and the optimalit condition we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimality function\n",
    "def imaml_objective(optimal_params, init_params, data):\n",
    "    x, y, f = data\n",
    "    y_pred = f(optimal_params, x)\n",
    "    regularisation_loss = 0\n",
    "    for p1, p2 in zip(optimal_params, init_params):\n",
    "        regularisation_loss += 0.5 * torch.sum((p1.view(-1) - p2.view(-1))**2)\n",
    "    loss = F.mse_loss(y_pred, y) + regularisation_loss\n",
    "    return loss \n",
    "\n",
    "# Optimality Condition is: the gradient w.r.t inner-loop optimal params is 0 (we achieve so by specifying argnums=0 in functorch.grad)\n",
    "# the argnums=1 specify which meta-parameter we want to backpropogate, in this case we want to backpropogate to the initial parameters\n",
    "# so we set it as 1.\n",
    "# You can also set argnums as (1,2) if you want to backpropogate through multiple meta parameters\n",
    "\n",
    "@implicit_diff.custom_root(functorch.grad(imaml_objective, argnums=0), argnums=1)\n",
    "def inner_solver(init_params_copy, init_params, data):\n",
    "    \"\"\"Solve ridge regression by conjugate gradient.\"\"\"\n",
    "    # inital functional optimizer based on torchopt\n",
    "    x, y, f = data\n",
    "    params = init_params_copy\n",
    "    optimizer = sgd(lr=2e-2)\n",
    "    opt_state = optimizer.init(params)\n",
    "    with torch.enable_grad():\n",
    "        # temporarily enable gradient computation for conducting the optimization\n",
    "        for i in range(100):\n",
    "            pred = f(params, x)   \n",
    "            loss = F.mse_loss(pred, y)                         # compute loss\n",
    "            regularisation_loss = 0\n",
    "            # compute regularisation loss\n",
    "            for p1, p2 in zip(params, init_params):\n",
    "                regularisation_loss += 0.5 * torch.sum((p1.view(-1) - p2.view(-1))**2)\n",
    "            final_loss = loss + regularisation_loss\n",
    "            grads = torch.autograd.grad(final_loss, params)                # compute gradients\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)  # get updates\n",
    "            params = TorchOpt.apply_updates(params, updates)       \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75c81-d479-4120-a73d-5b2b488358d0",
   "metadata": {},
   "source": [
    "In the next step, we consider a specific case for one layer neural network to fit the linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb95538b-1fd9-4ec8-9f57-6360bedc05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 4)\n",
    "w = torch.randn(4, 1)\n",
    "b = torch.randn(1)\n",
    "y = x @ w + b + torch.randn(20, 1) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1823a-2231-4471-bb68-cce7724f2578",
   "metadata": {},
   "source": [
    "We instantiate an one layer neural network, where the weights and bias are initialised with constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50a7bfe-ac69-4089-8cf8-3cbd69d6d4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1, bias=True)\n",
    "        nn.init.ones_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = Net(4)\n",
    "f, p = functorch.make_functional(model)\n",
    "data = (x, y, f)\n",
    "\n",
    "# clone function for \n",
    "def clone(p):\n",
    "    p_out = []\n",
    "    for item in p:\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            p_out.append(item.clone().detach_().requires_grad_(True))\n",
    "        else:\n",
    "            p_out.append(item)\n",
    "    return tuple(p_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c36c4-89e2-4a63-8213-63db6ee3b08e",
   "metadata": {},
   "source": [
    "We take the forward process by calling out the forward function, then we pass the optimal params into the outer-loop loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "115e79c6-911f-4743-a2ed-e50a71c3a813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_params = inner_solver(clone(p), p, data)\n",
    "\n",
    "outer_loss = f(optimal_params, x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812351-f635-496e-9732-c80831ac04a6",
   "metadata": {},
   "source": [
    "Finally, we can get the meta gradient as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bdcbe8d-2336-4f80-b124-eb43c5a2fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0582, -0.0163,  0.0379, -0.0265]]), tensor([0.2984]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(outer_loss, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchopt] *",
   "language": "python",
   "name": "conda-env-torchopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
