{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for implicit differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/5_Implicit_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f9865-dc02-43d4-be90-da1160c4e4dd",
   "metadata": {},
   "source": [
    "By treating the solution $\\phi^{\\star}$ as an implicit function of $\\theta$, the idea of implicit differentiation is to directly get analytical best-response derivatives $\\partial \\phi^{\\star}(\\theta)/ \\partial \\theta$ by implicit function theorem. This is suitable for algorithms when the inner-level optimal solution is achieved ${\\left. \\frac{\\partial F (\\phi, \\theta)}{\\partial \\phi} \\right\\rvert}_{\\phi = \\phi^{\\star}} = 0$ or reaches some stationary conditions $F (\\phi^{\\star}, \\theta) = 0$, such as [IMAML](https://arxiv.org/abs/1909.04630) and [DEQ](https://arxiv.org/abs/1909.01377)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4b9e1-115f-45ad-a9b3-ea338bcfe6dd",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to conduct implicit differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Functional API\n",
    "\n",
    "The basic functional API is `torchopt.diff.implicit.custom_root`, which is used as the decorator for the forward process implicit gradient procedures. Users are required to implement the stationary conditions for the inner-loop process, which will be used as the input of custom_root decorator. We show the pseudo code in the following part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4400b-a491-4f07-926c-c421ac5a2069",
   "metadata": {},
   "source": [
    "```python\n",
    "# Functional API for implicit gradient\n",
    "def stationary(params, meta_params, data):\n",
    "    # stationary condition construction\n",
    "    return stationary condition\n",
    "\n",
    "# Decorator that wraps the function\n",
    "# Optionally specify the linear solver (conjugate gradient or Neumann series)\n",
    "@torchopt.diff.implicit.custom_root(stationary, solve=linear_solver)\n",
    "def solve(params, meta_params, data):\n",
    "    # Forward optimization process for params\n",
    "    return optimal_params\n",
    "\n",
    "# Define params, meta params and get data\n",
    "params, meta_prams, data = ..., ..., ...\n",
    "optimal_params = solve(params, meta_params, data)\n",
    "loss = outer_loss(optimal_params)\n",
    "\n",
    "meta_grads = torch.autograd.grad(loss, meta_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "Here we use the example of [IMAML](https://arxiv.org/abs/1909.04630) as a real example. For IMAML, the inner-loop objective is described by the following equation.\n",
    "\n",
    "$$\n",
    "{\\mathcal{Alg}}^{\\star} \\left( \\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\text{tr}} \\right) = \\underset{\\phi'}{\\operatorname{\\arg \\min}} ~ G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\triangleq \\mathcal{L} \\left( \\boldsymbol{\\phi}', \\mathcal{D}_{i}^{\\text{tr}} \\right) + \\frac{\\lambda}{2} {\\left\\| \\boldsymbol{\\phi}' - \\boldsymbol{\\theta} \\right\\|}^{2}\n",
    "$$\n",
    "\n",
    "According to this function, we can define the forward function `inner_solver`, where we solve this equation based on sufficient gradient descents. For such inner-loop process, the optimality condition is that the gradient w.r.t inner-loop parameter is $0$.\n",
    "\n",
    "$$\n",
    "{\\left. \\nabla_{\\boldsymbol{\\phi}'} G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\right\\rvert}_{\\boldsymbol{\\phi}' = \\boldsymbol{\\phi}^{\\star}} = 0\n",
    "$$\n",
    "\n",
    "Thus we can define the optimality function by defining `imaml_objective` and make it first-order gradient w.r.t the inner-loop parameter as $0$. We achieve so by calling out `functorch.grad(imaml_objective, argnums=0)`. Finally, the forward function is decorated by the `@torchopt.diff.implicit.custom_root` decorator and the optimality condition we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner-loop objective function\n",
    "# The optimality function: grad(imaml_objective)\n",
    "def imaml_objective(params, meta_params, data):\n",
    "    x, y, fmodel = data\n",
    "    y_pred = fmodel(params, x)\n",
    "    regularization_loss = 0.0\n",
    "    for p1, p2 in zip(params, meta_params):\n",
    "        regularization_loss += 0.5 * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "    loss = F.mse_loss(y_pred, y) + regularization_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Optimality Condition is: the gradient w.r.t inner-loop optimal params is 0 (we achieve so by\n",
    "# specifying argnums=0 in functorch.grad) the argnums=1 specify which meta-parameter we want to\n",
    "# backpropogate, in this case we want to backpropogate to the initial parameters so we set it as 1.\n",
    "# You can also set argnums as (1, 2) if you want to backpropogate through multiple meta parameters\n",
    "\n",
    "# Here we pass argnums=1 to the custom_root. That means we want to compute the gradient of\n",
    "# optimal_params w.r.t. the 1-indexed argument in inner_solver, i.e., params.\n",
    "# torchopt.linear_solve.solve_normal_cg specify that we use the conjugate gradient based linear solver\n",
    "@torchopt.diff.implicit.custom_root(\n",
    "    functorch.grad(imaml_objective, argnums=0),  # optimality function\n",
    "    argnums=1,\n",
    "    solve=torchopt.linear_solve.solve_normal_cg(maxiter=5, atol=0),\n",
    ")\n",
    "def inner_solver(params, meta_params, data):\n",
    "    # Initial functional optimizer based on TorchOpt\n",
    "    x, y, fmodel = data\n",
    "    optimizer = torchopt.sgd(lr=2e-2)\n",
    "    opt_state = optimizer.init(params)\n",
    "    with torch.enable_grad():\n",
    "        # Temporarily enable gradient computation for conducting the optimization\n",
    "        for i in range(100):\n",
    "            pred = fmodel(params, x)\n",
    "            loss = F.mse_loss(pred, y)  # compute loss\n",
    "\n",
    "            # Compute regularization loss\n",
    "            regularization_loss = 0.0\n",
    "            for p1, p2 in zip(params, meta_params):\n",
    "                regularization_loss += 0.5 * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "            final_loss = loss + regularization_loss\n",
    "\n",
    "            grads = torch.autograd.grad(final_loss, params)  # compute gradients\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, inplace=True)  # get updates\n",
    "            params = torchopt.apply_updates(params, updates, inplace=True)\n",
    "\n",
    "    optimal_params = params\n",
    "    return optimal_params\n",
    "\n",
    "\n",
    "# torchopt.linear_solve.solve_inv specify that we use the Neumann Series inversion linear solver\n",
    "@torchopt.diff.implicit.custom_root(\n",
    "    functorch.grad(imaml_objective, argnums=0),  # optimality function\n",
    "    argnums=1,\n",
    "    solve=torchopt.linear_solve.solve_inv(ns=True, maxiter=100, alpha=0.1),\n",
    ")\n",
    "def inner_solver_inv_ns(params, meta_params, data):\n",
    "    # Initial functional optimizer based on TorchOpt\n",
    "    x, y, fmodel = data\n",
    "    optimizer = torchopt.sgd(lr=2e-2)\n",
    "    opt_state = optimizer.init(params)\n",
    "    with torch.enable_grad():\n",
    "        # Temporarily enable gradient computation for conducting the optimization\n",
    "        for i in range(100):\n",
    "            pred = fmodel(params, x)\n",
    "            loss = F.mse_loss(pred, y)  # compute loss\n",
    "\n",
    "            # Compute regularization loss\n",
    "            regularization_loss = 0.0\n",
    "            for p1, p2 in zip(params, meta_params):\n",
    "                regularization_loss += 0.5 * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "            final_loss = loss + regularization_loss\n",
    "\n",
    "            grads = torch.autograd.grad(final_loss, params)  # compute gradients\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, inplace=True)  # get updates\n",
    "            params = torchopt.apply_updates(params, updates, inplace=True)\n",
    "\n",
    "    optimal_params = params\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75c81-d479-4120-a73d-5b2b488358d0",
   "metadata": {},
   "source": [
    "In the next step, we consider a specific case for one layer neural network to fit the linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95538b-1fd9-4ec8-9f57-6360bedc05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(20, 4)\n",
    "w = torch.randn(4, 1)\n",
    "b = torch.randn(1)\n",
    "y = x @ w + b + 0.5 * torch.randn(20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1823a-2231-4471-bb68-cce7724f2578",
   "metadata": {},
   "source": [
    "We instantiate an one layer neural network, where the weights and bias are initialized with constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50a7bfe-ac69-4089-8cf8-3cbd69d6d4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1, bias=True)\n",
    "        nn.init.ones_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Net(4)\n",
    "fmodel, meta_params = functorch.make_functional(model)\n",
    "data = (x, y, fmodel)\n",
    "\n",
    "# Clone function for parameters\n",
    "def clone(params):\n",
    "    cloned = []\n",
    "    for item in params:\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            cloned.append(item.clone().detach_().requires_grad_(True))\n",
    "        else:\n",
    "            cloned.append(item)\n",
    "    return tuple(cloned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c36c4-89e2-4a63-8213-63db6ee3b08e",
   "metadata": {},
   "source": [
    "We take the forward process by calling out the forward function, then we pass the optimal params into the outer-loop loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115e79c6-911f-4743-a2ed-e50a71c3a813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_params = inner_solver(clone(meta_params), meta_params, data)\n",
    "\n",
    "outer_loss = fmodel(optimal_params, x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812351-f635-496e-9732-c80831ac04a6",
   "metadata": {},
   "source": [
    "Finally, we can get the meta gradient as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdcbe8d-2336-4f80-b124-eb43c5a2fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outer_loss, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ae8bb",
   "metadata": {},
   "source": [
    "Also we can switch to the Neumann Series inversion linear solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43df0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "optimal_params = inner_solver_inv_ns(clone(meta_params), meta_params, data)\n",
    "outer_loss = fmodel(optimal_params, x).mean()\n",
    "torch.autograd.grad(outer_loss, meta_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e67ea-b220-4a14-a1ea-4eb3c5f52b6b",
   "metadata": {},
   "source": [
    "## 2. OOP API\n",
    "\n",
    "The basic OOP class is the class `ImplicitMetaGradientModule`. We make the network as an `nn.Module` following a classical PyTorch style. Users need to define the stationary condition/objective function and the inner-loop solve function to enable implicit gradient computation. We show the pseudo code in the following part.\n",
    "\n",
    "```python\n",
    "from torchopt.diff.implicit import ImplicitMetaGradientModule\n",
    "\n",
    "# Inherited from the class ImplicitMetaGradientModule\n",
    "# Optionally specify the linear solver (conjugate gradient or Neumann series)\n",
    "class InnerNet(ImplicitMetaGradientModule, linear_solve=linear_solver):\n",
    "    def __init__(self, meta_module):\n",
    "        ...\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Forward process\n",
    "        ...\n",
    "\n",
    "    def optimality(self, batch, labels):\n",
    "        # Stationary condition construction for calculating implicit gradient\n",
    "        # NOTE: If this method is not implemented, it will be automatically derived from the\n",
    "        # gradient of the `objective` function.\n",
    "        ...\n",
    "\n",
    "    def objective(self, batch, labels):\n",
    "        # Define the inner-loop optimization objective\n",
    "        # NOTE: This method is optional if method `optimality` is implemented.\n",
    "        ...\n",
    "\n",
    "    def solve(self, batch, labels):\n",
    "        # Conduct the inner-loop optimization\n",
    "        ...\n",
    "        return self  # optimized module\n",
    "\n",
    "# Get meta_params and data\n",
    "meta_params, data = ..., ...\n",
    "inner_net = InnerNet()\n",
    "\n",
    "# Solve for inner-loop process related with the meta parameters\n",
    "optimal_inner_net = inner_net.solve(meta_params, *data)\n",
    "\n",
    "# Get outer-loss and solve for meta-gradient\n",
    "loss = outer_loss(optimal_inner_net)\n",
    "meta_grad = torch.autograd.grad(loss, meta_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29053597-62c4-423b-9a36-4d7e462ca922",
   "metadata": {},
   "source": [
    "For a custom network, users are required to define the meta-parameter (typically meta module's parameters) and inner-parameter (typically `self.parameters`) before calling the `solve` function. By default, `ImplicitMetaGradientModule` treats all `nn.Module` or `torch.Tensor` in the input as meta-parameter, while treats the rest as inner-parameter (including `nn.Module` defined in `__init__` or copied network).\n",
    "\n",
    "After calling the solve function, `ImplicitMetaGradientModule` will automatically calculate the implicit gradient defined in optimality function, and connect the gradient flow between meta-parameter and inner-parameter. Here we offer the example of implicit-maml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3999684-f4d3-4bc0-86ab-a7e803b2fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "from torchopt.diff.implicit import ImplicitMetaGradientModule\n",
    "\n",
    "\n",
    "class InnerNet(\n",
    "    ImplicitMetaGradientModule,\n",
    "    linear_solve=torchopt.linear_solve.solve_normal_cg(maxiter=5, atol=0),\n",
    "):\n",
    "    def __init__(self, meta_net, n_inner_iter, reg_param):\n",
    "        super().__init__()\n",
    "        # treated as meta-parameter\n",
    "        self.meta_net = meta_net\n",
    "        # Get a deepcopy, treated as inner-parameter\n",
    "        self.net = torchopt.module_clone(meta_net, by='deepcopy', detach_buffers=True)\n",
    "        self.n_inner_iter = n_inner_iter\n",
    "        self.reg_param = reg_param\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def objective(self, x, y):\n",
    "        # We do not implement the optimality conditions, so it will be automatically derived from\n",
    "        # the gradient of the `objective` function.\n",
    "        y_pred = self(x)\n",
    "        loss = F.mse_loss(y_pred, y)\n",
    "        regularization_loss = 0\n",
    "        for p1, p2 in zip(\n",
    "            self.parameters(),  # parameters of `self.net`\n",
    "            self.meta_parameters(),  # parameters of `self.meta_net`\n",
    "        ):\n",
    "            regularization_loss += (\n",
    "                0.5 * self.reg_param * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "            )\n",
    "        return loss + regularization_loss\n",
    "\n",
    "    def solve(self, x, y):\n",
    "        params = tuple(self.parameters())\n",
    "        inner_optim = torchopt.SGD(params, lr=2e-2)\n",
    "        with torch.enable_grad():\n",
    "            # Temporarily enable gradient computation for conducting the optimization\n",
    "            for _ in range(self.n_inner_iter):\n",
    "                loss = self.objective(x, y)\n",
    "                inner_optim.zero_grad()\n",
    "                # NOTE: The parameter inputs should be explicitly specified in `backward` function\n",
    "                # as argument `inputs`. Otherwise, if not provided, the gradient is accumulated into\n",
    "                # all the leaf Tensors (including the meta-parameters) that were used to compute the\n",
    "                # objective output. Alternatively, please use `torch.autograd.grad` instead.\n",
    "                loss.backward(inputs=params)  # backward pass in inner-loop\n",
    "                inner_optim.step()  # update inner parameters\n",
    "        return self\n",
    "        # torchopt get the implicit gradient from self.parameters() to meta_net.parameters()\n",
    "\n",
    "\n",
    "# Initialize the meta network\n",
    "meta_net = Net(4)\n",
    "inner_net = InnerNet(meta_net, 100, reg_param=1)\n",
    "\n",
    "# Solve for inner-loop\n",
    "optimal_inner_net = inner_net.solve(x, y)\n",
    "outer_loss = optimal_inner_net(x).mean()\n",
    "\n",
    "# Derive the meta gradient\n",
    "torch.autograd.grad(outer_loss, meta_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e23e8-8aa1-4798-8483-655109460ea2",
   "metadata": {},
   "source": [
    "Another example for DEQ (fixed-point iteration in the inner-loop) can also be implemented using the OOP API. DEQ has the following fixed-point optimality conditions (inner-loop objective).\n",
    "$$\n",
    "F \\left( \\boldsymbol{\\theta}, x^{\\star} \\right) = x^{\\star}\n",
    "$$\n",
    "We are going to derive the implicit gradient from inner-parameter $x^{\\star}$ to meta-parameter $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8386ccd1-783f-46bf-b87d-afbfb683b215",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4]) and output[0] has a shape of torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3648664/1571227362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# derive the meta gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    275\u001b[0m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/waterhorse/torchopt/torchopt/diff/implicit/decorator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0moutput_is_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_is_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                     \u001b[0msolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/waterhorse/torchopt/torchopt/diff/implicit/decorator.py\u001b[0m in \u001b[0;36m_root_vjp\u001b[0;34m(optimality_fn, solution, args, grad_outputs, output_is_tensor, argnums, solve)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# v = -grad_outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTupleOfTensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type,assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTupleOfTensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     masked_optimality_fn = MaskedOptimalityFn(\n",
      "\u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/waterhorse/torchopt/torchopt/linear_solve/normal_cg.py\u001b[0m in \u001b[0;36m_solve_normal_cg\u001b[0;34m(matvec, b, ridge, init, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mexample_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mrmatvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_rmatvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_x\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (x) -> A.T @ x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mnormal_matvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_normal_matvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (x) -> A.T @ A @ x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/waterhorse/torchopt/torchopt/linear_solve/utils.py\u001b[0m in \u001b[0;36mmake_rmatvec\u001b[0;34m(matvec, example_x)\u001b[0m\n\u001b[1;32m     44\u001b[0m ) -> Callable[[TensorTree], TensorTree]:\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m\"\"\"Returns a function that computes ``rmatvec(y) = A.T @ y`` from ``matvec(x) = A @ x``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/functorch/_src/eager_transforms.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(func, has_aux, *primals)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mprimals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_all_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mdiff_primals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_differentiable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mprimals_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff_primals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nasdata/liubo/liubo/marl_xidong/waterhorse/torchopt/torchopt/diff/implicit/decorator.py\u001b[0m in \u001b[0;36mmatvec\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmatvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTupleOfTensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTupleOfTensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moptimality_cond_vjp_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# The solution of A^T u = v, where\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/functorch/_src/eager_transforms.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(cotangents, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    276\u001b[0m                     f'primal output: {treespec_pprint(primals_out_spec)}')\n\u001b[1;32m    277\u001b[0m             result = _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n\u001b[0;32m--> 278\u001b[0;31m                                     retain_graph=retain_graph, create_graph=create_graph)\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/functorch/_src/eager_transforms.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     90\u001b[0m                                       \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                                       \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                       allow_unused=True)\n\u001b[0m\u001b[1;32m     93\u001b[0m     grad_inputs = tuple(torch.zeros_like(inp) if gi is None else gi\n\u001b[1;32m     94\u001b[0m                         for gi, inp in zip(grad_inputs, inputs))\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mgrad_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mgrad_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_grads_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/quantum/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                        \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and output[\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                        \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] has a shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                        + str(out.shape) + \".\")\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 raise RuntimeError(\"For complex Tensors, both grad_output and output\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4]) and output[0] has a shape of torch.Size([4])."
     ]
    }
   ],
   "source": [
    "from torchopt.diff.implicit import ImplicitMetaGradientModule\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, dim))\n",
    "        # nn.init.ones_(self.fc.weight)\n",
    "        # nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class InnerNet(\n",
    "    ImplicitMetaGradientModule,\n",
    "    linear_solve=torchopt.linear_solve.solve_normal_cg(maxiter=5, atol=0),\n",
    "):\n",
    "    def __init__(self, meta_net):\n",
    "        super().__init__()\n",
    "        self.meta_net = meta_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.meta_net(x)\n",
    "\n",
    "    def optimality(self):\n",
    "        return tuple(self.x - self.forward(self.x))\n",
    "\n",
    "    def register_x(self, x):\n",
    "        # inner parameter should be the parameter of self so we set it here\n",
    "        # so self.x belongs to self.parameters()\n",
    "        self.x = nn.Parameter(x)\n",
    "\n",
    "    def solve(\n",
    "        self,\n",
    "    ):\n",
    "        # conduct iterative fixed point process\n",
    "        for _ in range(10):\n",
    "            self.x = nn.Parameter(self(self.x))\n",
    "        return self\n",
    "        # torchopt get the implicit gradient from self.x to meta_net.parameters()\n",
    "\n",
    "\n",
    "# initialize meta network\n",
    "meta_net = Net(4)\n",
    "x = torch.randn(1, 4)\n",
    "inner_net = InnerNet(\n",
    "    meta_net,\n",
    ")\n",
    "# register inner-parameter before calling solve\n",
    "# Must define the self.x (self.parameter) before calling the solve function\n",
    "# do not set x as the input for solve function\n",
    "inner_net.register_x(x)\n",
    "# solve for inner-loop\n",
    "optimal_inner_net = inner_net.solve()\n",
    "outer_loss = optimal_inner_net.x.mean()\n",
    "\n",
    "# derive the meta gradient\n",
    "torch.autograd.grad(outer_loss, meta_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcccb3b-f72d-4fd3-9611-5c1d92829a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8cc1ff2cbc47027bf9993941710d9ab9175f14080903d9c7c432ee63d681da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
