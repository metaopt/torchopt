{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for implicit differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/5_Implicit_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01214059-78d9-4cde-9b08-007c2eb078b3",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to conduct implicit differentiation. Here we use the example of [IMAML](https://arxiv.org/abs/1909.04630) as the illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Basic API\n",
    "\n",
    "The basic API is `torchopt.diff.implicit.custom_root`, which is used as the decorator for the forward process implicit gradient procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "## 1.1 Forward Process, Backward Process with Optimality Conditions\n",
    "For IMAML, the inner-loop objective is described by the following equation.\n",
    "\n",
    "$$\n",
    "{\\mathcal{Alg}}^{\\star} \\left( \\boldsymbol{\\theta}, \\mathcal{D}_{i}^{\\text{tr}} \\right) = \\underset{\\phi' \\in \\Phi}{\\operatorname{\\arg \\min}} ~ G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\triangleq \\mathcal{L} \\left( \\boldsymbol{\\phi}', \\mathcal{D}_{i}^{\\text{tr}} \\right) + \\frac{\\lambda}{2} {\\left\\| \\boldsymbol{\\phi}' - \\boldsymbol{\\theta} \\right\\|}^{2}\n",
    "$$\n",
    "\n",
    "According to this function, we can define the forward function `inner_solver`, where we solve this equation based on sufficient gradient descents. For such inner-loop process, the optimality condition is that the gradient w.r.t inner-loop parameter is $0$.\n",
    "\n",
    "$$\n",
    "{\\left. \\nabla_{\\boldsymbol{\\phi}'} G \\left( \\boldsymbol{\\phi}', \\boldsymbol{\\theta} \\right) \\right|}_{\\boldsymbol{\\phi}' = \\boldsymbol{\\phi}^{\\star}} = 0\n",
    "$$\n",
    "\n",
    "Thus we can define the optimality function by defining `imaml_objective` and make it first-order gradient w.r.t the inner-loop parameter as $0$. We achieve so by calling out `functorch.grad(imaml_objective, argnums=0)`. Finally, the forward function is decorated by the `@torchopt.diff.implicit.custom_root` decorator and the optimality condition we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimality function\n",
    "def imaml_objective(params, meta_params, data):\n",
    "    x, y, fmodel = data\n",
    "    y_pred = fmodel(params, x)\n",
    "    regularization_loss = 0.0\n",
    "    for p1, p2 in zip(params, meta_params):\n",
    "        regularization_loss += 0.5 * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "    loss = F.mse_loss(y_pred, y) + regularization_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Optimality Condition is: the gradient w.r.t inner-loop optimal params is 0 (we achieve so by\n",
    "# specifying argnums=0 in functorch.grad) the argnums=1 specify which meta-parameter we want to\n",
    "# backpropogate, in this case we want to backpropogate to the initial parameters so we set it as 1.\n",
    "# You can also set argnums as (1, 2) if you want to backpropogate through multiple meta parameters\n",
    "\n",
    "# Here we pass argnums=1 to the custom_root. That means we want to compute the gradient of\n",
    "# optimal_params w.r.t. the 1-indexed argument in inner_solver, i.e., params.\n",
    "@torchopt.diff.implicit.custom_root(functorch.grad(imaml_objective, argnums=0), argnums=1)\n",
    "def inner_solver(params, meta_params, data):\n",
    "    \"\"\"Solve ridge regression by conjugate gradient.\"\"\"\n",
    "    # Initial functional optimizer based on TorchOpt\n",
    "    x, y, fmodel = data\n",
    "    optimizer = torchopt.sgd(lr=2e-2)\n",
    "    opt_state = optimizer.init(params)\n",
    "    with torch.enable_grad():\n",
    "        # Temporarily enable gradient computation for conducting the optimization\n",
    "        for i in range(100):\n",
    "            pred = fmodel(params, x)\n",
    "            loss = F.mse_loss(pred, y)  # compute loss\n",
    "\n",
    "            # Compute regularization loss\n",
    "            regularization_loss = 0.0\n",
    "            for p1, p2 in zip(params, meta_params):\n",
    "                regularization_loss += 0.5 * torch.sum(torch.square(p1.view(-1) - p2.view(-1)))\n",
    "            final_loss = loss + regularization_loss\n",
    "\n",
    "            grads = torch.autograd.grad(final_loss, params)  # compute gradients\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, inplace=True)  # get updates\n",
    "            params = torchopt.apply_updates(params, updates, inplace=True)\n",
    "\n",
    "    optimal_params = params\n",
    "    return optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75c81-d479-4120-a73d-5b2b488358d0",
   "metadata": {},
   "source": [
    "In the next step, we consider a specific case for one layer neural network to fit the linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95538b-1fd9-4ec8-9f57-6360bedc05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(20, 4)\n",
    "w = torch.randn(4, 1)\n",
    "b = torch.randn(1)\n",
    "y = x @ w + b + 0.5 * torch.randn(20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1823a-2231-4471-bb68-cce7724f2578",
   "metadata": {},
   "source": [
    "We instantiate an one layer neural network, where the weights and bias are initialised with constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50a7bfe-ac69-4089-8cf8-3cbd69d6d4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1, bias=True)\n",
    "        nn.init.ones_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Net(4)\n",
    "fmodel, meta_params = functorch.make_functional(model)\n",
    "data = (x, y, fmodel)\n",
    "\n",
    "# clone function for parameters\n",
    "def clone(params):\n",
    "    cloned = []\n",
    "    for item in params:\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            cloned.append(item.clone().detach_().requires_grad_(True))\n",
    "        else:\n",
    "            cloned.append(item)\n",
    "    return tuple(cloned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c36c4-89e2-4a63-8213-63db6ee3b08e",
   "metadata": {},
   "source": [
    "We take the forward process by calling out the forward function, then we pass the optimal params into the outer-loop loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115e79c6-911f-4743-a2ed-e50a71c3a813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_params = inner_solver(clone(meta_params), meta_params, data)\n",
    "\n",
    "outer_loss = fmodel(optimal_params, x).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812351-f635-496e-9732-c80831ac04a6",
   "metadata": {},
   "source": [
    "Finally, we can get the meta gradient as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdcbe8d-2336-4f80-b124-eb43c5a2fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0369,  0.0248,  0.0347,  0.0067]]), tensor([0.3156]))\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outer_loss, meta_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torchopt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8cc1ff2cbc47027bf9993941710d9ab9175f14080903d9c7c432ee63d681da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
