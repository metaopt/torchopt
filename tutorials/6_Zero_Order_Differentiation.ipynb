{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8850c832-3b54-4971-8ee0-2cd64b585ea8",
   "metadata": {},
   "source": [
    "# TorchOpt for Zero-Order Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547376",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/metaopt/torchopt/blob/main/tutorials/6_Zero_Order_Differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f9865-dc02-43d4-be90-da1160c4e4dd",
   "metadata": {},
   "source": [
    "When the inner-loop process is non-differentiable or one wants to eliminate the heavy computation burdens in the previous two modes (brought by Hessian), one can choose ZD. ZD typically gets gradients based on zero-order estimation, such as finite-difference, or Evolutionary Strategy.\n",
    "\n",
    "TorchOpt offers API for ES-based differentiation. Instead of optimizing the objective $F$, ES optimizes a Gaussion smoothing objective defined as $\\tilde{f}_{\\sigma} (\\theta) = \\mathbb{E}_{{z} \\sim \\mathcal{N}( {0}, {I}_d )} [ f ({\\theta} + \\sigma \\, z) ]$, where $\\sigma$ denotes precision. The gradient of such objective is $\\nabla_\\theta \\tilde{f}_{\\sigma} (\\theta) = \\frac{1}{\\sigma} \\mathbb{E}_{{z} \\sim \\mathcal{N}( {0}, {I}_d )} [ f({\\theta} + \\sigma \\, z) \\cdot z ]$. Refer to [ES-MAML](https://arxiv.org/pdf/1910.01215.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4b9e1-115f-45ad-a9b3-ea338bcfe6dd",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be used to ES-based differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f13ae67-e328-409f-84a8-1fc425c03a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaac49-4b94-4900-9bb5-a39057ac8b21",
   "metadata": {},
   "source": [
    "## 1. Functional API\n",
    "\n",
    "The basic functional API is `torchopt.diff.zero_order.zero_order`, which is used as the decorator for the forward process zero-order gradient procedures. Users are required to implement the noise sampling function, which will be used as the input of zero_order decorator. Here we show the specific meaning for each parameter used in the decorator.\n",
    "\n",
    "- `distribution` for noise sampling distribution\n",
    "- `method` for different kind of algorithms, we support `'naive'` ([ES-RL](https://arxiv.org/abs/1703.03864)), `'forward'` ([Forward-FD](http://proceedings.mlr.press/v80/choromanski18a/choromanski18a.pdf)), and `'antithetic'` ([antithetic](https://d1wqtxts1xzle7.cloudfront.net/75609515/coredp2011_1web-with-cover-page-v2.pdf?Expires=1670215467&Signature=RfP~mQhhhI7aGknwXbRBgSggFrKuNTPYdyUSdMmfTxOa62QoOJAm-Xhr3F1PLyjUQc2JVxmKIKGGuyYvyfCTpB31dfmMtuVQxZMWVF-SfErTN05SliC93yjA1x1g2kjhn8bkBFdQqGl~1RQSKnhj88BakgSeDNzyCxwbD5VgR89BXRs4YIK5RBIKYtgLhoyz5jar7wHS3TJhRzs3WNeTIAjAmLqJ068oGFZ0Jr7maGquTe3w~8LEEIprJ6cyCMc6b1UUJkmwjNq0RLTVbxgFjfi4Z9kyxyJB9IOS1J25OOON4jfwh5JlXS7MVskuONUyHJim1TQ8OwCraKlBsQLPQw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)).\n",
    "- `argnums` specifies which parameter we want to trace the meta-gradient.\n",
    "- `sigma` is for precision.\n",
    "- `num_samples` specifies how many times we want to conduct the sampling.\n",
    "\n",
    "We show the pseudo code in the following part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4400b-a491-4f07-926c-c421ac5a2069",
   "metadata": {},
   "source": [
    "```python\n",
    "# Functional API for zero-order differentiation\n",
    "# 1. Customize the noise distribution via a distribution class\n",
    "class Distribution:\n",
    "    def sample(self, sample_shape = torch.Size()):\n",
    "        # sampling function for noise\n",
    "        return noise_batch\n",
    "\n",
    "distribution = Distribution()\n",
    "\n",
    "# 2. Customize the noise distribution via a sampling function\n",
    "def distribution(sample_shape = torch.Size()):\n",
    "    # sampling function for noise\n",
    "    return noise_batch\n",
    "\n",
    "# 3. Distribution can also be an instance of `torch.distributions.Distribution`, e.g., `torch.distributions.Normal(...)`\n",
    "distribution = torch.distributions.Normal(loc=0, scale=1)\n",
    "\n",
    "# Decorator that wraps the function\n",
    "@torchopt.diff.zero_order(distribution=distribution, method='naive', argnums=0, sigma=0.01, num_samples=100)\n",
    "def forward(params, data):\n",
    "    # Forward optimization process for params\n",
    "    return output\n",
    "\n",
    "# Define params and get data\n",
    "params, data = ..., ...\n",
    "loss = forward(params, data)\n",
    "\n",
    "meta_grads = torch.autograd.grad(loss, params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef87df-2164-4f1d-8919-37a6fbdc5011",
   "metadata": {},
   "source": [
    "Here we use the example of a linear layer as an example, note that this is just an example to show linear layer can work with ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d623b2f-48ee-4df6-a2ce-cf306b4c9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001: tensor(0.0269, grad_fn=<ZeroOrderBackward>)\n",
      "002: tensor(0.0246, grad_fn=<ZeroOrderBackward>)\n",
      "003: tensor(0.0225, grad_fn=<ZeroOrderBackward>)\n",
      "004: tensor(0.0205, grad_fn=<ZeroOrderBackward>)\n",
      "005: tensor(0.0187, grad_fn=<ZeroOrderBackward>)\n",
      "006: tensor(0.0171, grad_fn=<ZeroOrderBackward>)\n",
      "007: tensor(0.0156, grad_fn=<ZeroOrderBackward>)\n",
      "008: tensor(0.0144, grad_fn=<ZeroOrderBackward>)\n",
      "009: tensor(0.0134, grad_fn=<ZeroOrderBackward>)\n",
      "010: tensor(0.0128, grad_fn=<ZeroOrderBackward>)\n",
      "011: tensor(0.0122, grad_fn=<ZeroOrderBackward>)\n",
      "012: tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "013: tensor(0.0120, grad_fn=<ZeroOrderBackward>)\n",
      "014: tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "015: tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "016: tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "017: tensor(0.0121, grad_fn=<ZeroOrderBackward>)\n",
      "018: tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "019: tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "020: tensor(0.0118, grad_fn=<ZeroOrderBackward>)\n",
      "021: tensor(0.0115, grad_fn=<ZeroOrderBackward>)\n",
      "022: tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "023: tensor(0.0117, grad_fn=<ZeroOrderBackward>)\n",
      "024: tensor(0.0116, grad_fn=<ZeroOrderBackward>)\n",
      "025: tensor(0.0113, grad_fn=<ZeroOrderBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "fmodel, params = functorch.make_functional(torch.nn.Linear(32, 1))\n",
    "x = torch.randn(64, 32) * 0.1\n",
    "y = torch.randn(64) * 0.1\n",
    "distribution = torch.distributions.Normal(loc=0, scale=1)\n",
    "\n",
    "\n",
    "@torchopt.diff.zero_order.zero_order(\n",
    "    distribution=distribution, method='forward', argnums=0, sigma=0.01, num_samples=1000\n",
    ")\n",
    "def forward_process(params, fn, x, y):\n",
    "    y_pred = fn(params, x)\n",
    "    loss = torch.mean((y - y_pred) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimizer = torchopt.adam(lr=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for i in range(25):\n",
    "    opt_state = optimizer.init(params)  # init optimizer\n",
    "    loss = forward_process(params, fmodel, x, y)  # compute loss\n",
    "\n",
    "    grads = torch.autograd.grad(loss, params)  # compute gradients\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)  # get updates\n",
    "    params = torchopt.apply_updates(params, updates)  # update network parameters\n",
    "\n",
    "    print(f'{i + 1:03d}: {loss!r}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torchopt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8cc1ff2cbc47027bf9993941710d9ab9175f14080903d9c7c432ee63d681da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
